{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\",low_memory=False)\n",
    "test_df = pd.read_csv(\"test.csv\",low_memory=False)\n",
    "store_df = pd.read_csv(\"store.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.merge(left = train_df, right = store_df, how = 'inner', left_on = 'Store', right_on = 'Store')\n",
    "test_merged = pd.merge(left = test_df, right = store_df, how = 'inner', left_on = 'Store', right_on = 'Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the training and testing dataset into feature and target variables\n",
    "train_feature = train_merged.drop(['Sales'], axis = 1) \n",
    "train_target  = train_merged[['Sales']]\n",
    "test_feature = test_merged.drop(['Id'], axis = 1) \n",
    "test_feature['Customers'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_addition(col):\n",
    "    col['Date'] = pd.to_datetime(col.Date)\n",
    "    col['Month'] = col.Date.dt.month.to_list()\n",
    "    col['Year'] = col.Date.dt.year.to_list()\n",
    "    col['Day'] = col.Date.dt.day.to_list()\n",
    "    col['WeekOfYear'] = col.Date.dt.weekofyear.to_list()\n",
    "    col['DayOfWeek'] = col.Date.dt.dayofweek.to_list()\n",
    "    col['weekday'] = 1        # Initialize the column with default value of 1\n",
    "    col.loc[col['DayOfWeek'] == 5, 'weekday'] = 0\n",
    "    col.loc[col['DayOfWeek'] == 6, 'weekday'] = 0\n",
    "    return col\n",
    "\n",
    "train_feature = column_addition(train_feature)\n",
    "test_feature = column_addition(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = []\n",
    "numerical = []\n",
    "timestamp = []\n",
    "\n",
    "for col in train_feature.columns:\n",
    "    if train_feature[col].dtype == object:\n",
    "        categorical.append(col)\n",
    "    elif train_feature[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "        numerical.append(col)\n",
    "    else:\n",
    "        timestamp.append(col)\n",
    "\n",
    "    # Keep selected columns only\n",
    "my_cols = categorical + numerical + timestamp\n",
    "train_feature = train_feature[my_cols].copy()\n",
    "test_feature = test_feature[my_cols].copy()\n",
    "feature = pd.concat([train_feature, test_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As can be seen from the info function,the year, month and week related columns are in float. CHanging that to integers\n",
    "\n",
    "feature.CompetitionOpenSinceMonth = feature.CompetitionOpenSinceMonth.astype('Int64') \n",
    "feature.CompetitionOpenSinceYear = feature.CompetitionOpenSinceYear.astype('Int64')\n",
    "feature.Promo2SinceWeek = feature.Promo2SinceWeek.astype('Int64') \n",
    "feature.Promo2SinceYear = feature.Promo2SinceYear.astype('Int64')\n",
    "feature[\"StateHoliday\"].loc[feature[\"StateHoliday\"] == 0] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treating missing values for numerical datatype\n",
    "for i in ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']:\n",
    "    feature[i] = feature[i].fillna((int(feature[i].median()))) \n",
    "\n",
    "# treating missing values for categorical datatype\n",
    "feature['Open'] = feature['Open'].fillna(feature['Open'].mode()[0])\n",
    "feature['PromoInterval'] = feature['PromoInterval'].fillna(feature['PromoInterval'].mode()[0])\n",
    "\n",
    "pm = pd.get_dummies(feature['PromoInterval'],prefix='PromoInterval')\n",
    "st = pd.get_dummies(feature['StoreType'],prefix='StoreType')\n",
    "assort = pd.get_dummies(feature['Assortment'],prefix='Assortment')\n",
    "state_holiday = pd.get_dummies(feature['StateHoliday'],prefix='StateHoliday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [feature,state_holiday,st,pm,assort]\n",
    "feature = pd.concat(final,axis=1, join='inner')\n",
    "\n",
    "feature = feature.drop(columns=['StoreType','PromoInterval','Assortment','StateHoliday','Date'])\n",
    "#min max normalization\n",
    "feature =(feature-feature.min())/(feature.max()-feature.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating training and testing back\n",
    "train_df_feature = feature.iloc[:len(train_feature), ]\n",
    "test_df_feature = feature.iloc[len(train_feature):, :]\n",
    "\n",
    "test_df_feature = test_df_feature.drop(columns=\"Customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_copy = train_df_feature.copy()\n",
    "train_copy['Sales'] = train_target\n",
    "\n",
    "#comment this code if you don't want sales to be normalized\n",
    "#train_copy['Sales'] =(train_copy['Sales']-train_copy['Sales'].min())/(train_copy['Sales'].max()-train_copy['Sales'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_copy.drop(columns=['Sales','Customers'])\n",
    "y      = np.log1p(train_copy[\"Sales\"])\n",
    "X_test = test_df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X      = X.to_numpy()\n",
    "y      = y.to_numpy().reshape(-1, 1)\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 4.68353 | val_0_rmse: 1.66069 |  0:00:30s\n",
      "epoch 1  | loss: 0.15195 | val_0_rmse: 0.51531 |  0:01:01s\n",
      "epoch 2  | loss: 0.13238 | val_0_rmse: 0.52031 |  0:01:33s\n",
      "epoch 3  | loss: 0.13127 | val_0_rmse: 0.38754 |  0:02:04s\n",
      "epoch 4  | loss: 0.12402 | val_0_rmse: 0.35397 |  0:02:35s\n",
      "epoch 5  | loss: 0.12174 | val_0_rmse: 0.34217 |  0:03:05s\n",
      "epoch 6  | loss: 0.11751 | val_0_rmse: 0.334   |  0:03:36s\n",
      "epoch 7  | loss: 0.11459 | val_0_rmse: 0.33872 |  0:04:07s\n",
      "epoch 8  | loss: 0.11609 | val_0_rmse: 0.33114 |  0:04:38s\n",
      "epoch 9  | loss: 0.11255 | val_0_rmse: 0.3304  |  0:05:09s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_rmse = 0.3304\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 4.78547 | val_0_rmse: 2.68846 |  0:00:31s\n",
      "epoch 1  | loss: 0.14747 | val_0_rmse: 1.09913 |  0:01:02s\n",
      "epoch 2  | loss: 0.13524 | val_0_rmse: 0.40476 |  0:01:34s\n",
      "epoch 3  | loss: 0.12666 | val_0_rmse: 0.34904 |  0:02:05s\n",
      "epoch 4  | loss: 0.12292 | val_0_rmse: 0.35827 |  0:02:36s\n",
      "epoch 5  | loss: 0.12164 | val_0_rmse: 0.34135 |  0:03:07s\n",
      "epoch 6  | loss: 0.11863 | val_0_rmse: 0.35541 |  0:03:38s\n",
      "epoch 7  | loss: 0.14088 | val_0_rmse: 0.35131 |  0:04:10s\n",
      "epoch 8  | loss: 0.12176 | val_0_rmse: 0.34505 |  0:04:41s\n",
      "epoch 9  | loss: 0.11802 | val_0_rmse: 0.33804 |  0:05:12s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_rmse = 0.33804\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 4.79737 | val_0_rmse: 2.44541 |  0:00:31s\n",
      "epoch 1  | loss: 0.14367 | val_0_rmse: 0.84865 |  0:01:02s\n",
      "epoch 2  | loss: 0.13321 | val_0_rmse: 0.38632 |  0:01:33s\n",
      "epoch 3  | loss: 0.12969 | val_0_rmse: 0.35585 |  0:02:04s\n",
      "epoch 4  | loss: 0.12465 | val_0_rmse: 0.34505 |  0:02:36s\n",
      "epoch 5  | loss: 0.12215 | val_0_rmse: 0.3453  |  0:03:07s\n",
      "epoch 6  | loss: 0.12123 | val_0_rmse: 0.3398  |  0:03:38s\n",
      "epoch 7  | loss: 0.12095 | val_0_rmse: 0.34644 |  0:04:09s\n",
      "epoch 8  | loss: 0.12238 | val_0_rmse: 0.34206 |  0:04:40s\n",
      "epoch 9  | loss: 0.11824 | val_0_rmse: 0.33589 |  0:05:12s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_rmse = 0.33589\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 4.69276 | val_0_rmse: 2.89934 |  0:00:31s\n",
      "epoch 1  | loss: 0.14805 | val_0_rmse: 1.02032 |  0:01:02s\n",
      "epoch 2  | loss: 0.12814 | val_0_rmse: 0.52849 |  0:01:34s\n",
      "epoch 3  | loss: 0.12428 | val_0_rmse: 0.35929 |  0:02:06s\n",
      "epoch 4  | loss: 0.1231  | val_0_rmse: 0.34466 |  0:02:40s\n",
      "epoch 5  | loss: 0.11867 | val_0_rmse: 0.33897 |  0:03:12s\n",
      "epoch 6  | loss: 0.11755 | val_0_rmse: 0.33944 |  0:03:43s\n",
      "epoch 7  | loss: 0.11552 | val_0_rmse: 0.33442 |  0:04:14s\n",
      "epoch 8  | loss: 0.1131  | val_0_rmse: 0.33253 |  0:04:45s\n",
      "epoch 9  | loss: 0.11205 | val_0_rmse: 0.33471 |  0:05:17s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_0_rmse = 0.33253\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 4.57772 | val_0_rmse: 2.3107  |  0:00:31s\n",
      "epoch 1  | loss: 0.14788 | val_0_rmse: 0.86475 |  0:01:03s\n",
      "epoch 2  | loss: 0.13028 | val_0_rmse: 0.44732 |  0:01:34s\n",
      "epoch 3  | loss: 0.12576 | val_0_rmse: 0.3679  |  0:02:06s\n",
      "epoch 4  | loss: 0.12369 | val_0_rmse: 0.34523 |  0:02:38s\n",
      "epoch 5  | loss: 0.11821 | val_0_rmse: 0.34442 |  0:03:10s\n",
      "epoch 6  | loss: 0.11697 | val_0_rmse: 0.33636 |  0:03:41s\n",
      "epoch 7  | loss: 0.11481 | val_0_rmse: 0.33451 |  0:04:13s\n",
      "epoch 8  | loss: 0.11559 | val_0_rmse: 0.3325  |  0:04:45s\n",
      "epoch 9  | loss: 0.11417 | val_0_rmse: 0.33423 |  0:05:16s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_0_rmse = 0.3325\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "predictions_array =[]\n",
    "CV_score_array    =[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    regressor = TabNetRegressor(verbose=1,seed=42)\n",
    "    regressor.fit(X_train=X_train, y_train=y_train,\n",
    "              eval_set=[(X_valid, y_valid)],\n",
    "              patience=10, max_epochs=10,batch_size=10000,\n",
    "              eval_metric=['rmse'])\n",
    "    CV_score_array.append(regressor.best_cost)\n",
    "    predictions_array.append(np.expm1(regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.mean(predictions_array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV score is 0.33387\n"
     ]
    }
   ],
   "source": [
    "print(\"The CV score is %.5f\" % np.mean(CV_score_array,axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0984727e+03]\n",
      " [7.3432881e+03]\n",
      " [7.7243657e+03]\n",
      " ...\n",
      " [6.7809009e+03]\n",
      " [1.8635247e-02]\n",
      " [5.0073926e+03]]\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41088\n"
     ]
    }
   ],
   "source": [
    "print(predictions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv',low_memory=False)\n",
    "\n",
    "submission['Sales'] = predictions\n",
    "submission.to_csv('submission_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
